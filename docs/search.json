[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi.I’m Subin Park. I am currently Junior in University. I’m majoring in Big Data. and this homepage is for data mining, one of the subjects I’m learning. I plan to upload what I learn in the future, so plz look forward to it:>"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Quiz_2\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSubin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz_3\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSubin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz_4\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSubin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz_5\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSubin\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz_1\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2023\n\n\nSubin\n\n\n\n\n\n\n  \n\n\n\n\nData_mining :1.Numpy\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2023\n\n\nSubin\n\n\n\n\n\n\n  \n\n\n\n\nMy first New post\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nSubin\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html",
    "href": "posts/first_numpy/first_lect.html",
    "title": "Data_mining :1.Numpy",
    "section": "",
    "text": "*넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.”\nnumpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다:"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.zeros",
    "href": "posts/first_numpy/first_lect.html#np.zeros",
    "title": "Data_mining :1.Numpy",
    "section": "np.zeros",
    "text": "np.zeros\nzeros 함수는 0으로 채워진 배열을 만듭니다:\n  np.zeros(5)\narray([0., 0., 0., 0., 0.])\n2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 \\(3 \\times 4\\) 크기의 행렬입니다:\n  np.zeros((3,4))\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#용어",
    "href": "posts/first_numpy/first_lect.html#용어",
    "title": "Data_mining :1.Numpy",
    "section": "용어",
    "text": "용어\n\n넘파이에서 각 차원을 축(axis) 이라고 합니다\n축의 개수를 랭크(rank) 라고 합니다.\n\n예를 들어, 위의 \\(3 \\times 4\\) 행렬은 랭크 2인 배열입니다(즉 2차원입니다).\n첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다.\n\n배열의 축 길이를 배열의 크기(shape)라고 합니다.\n\n예를 들어, 위 행렬의 크기는 (3, 4)입니다.\n랭크는 크기의 길이와 같습니다.\n\n배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, \\(3 \\times 4=12\\)).\n\na = np.zeros((3,4))\na\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])\na.shape\n(3, 4)\na.ndim  # len(a.shape)와 같습니다\n2\na.size\n12"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#n-차원-배열",
    "href": "posts/first_numpy/first_lect.html#n-차원-배열",
    "title": "Data_mining :1.Numpy",
    "section": "N-차원 배열",
    "text": "N-차원 배열\n임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다:\nnp.zeros((2,3,4))\narray([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#배열-타입",
    "href": "posts/first_numpy/first_lect.html#배열-타입",
    "title": "Data_mining :1.Numpy",
    "section": "배열 타입",
    "text": "배열 타입\n넘파이 배열의 타입은 ndarray입니다:\ntype(np.zeros((3,4)))\nnumpy.ndarray"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.ones",
    "href": "posts/first_numpy/first_lect.html#np.ones",
    "title": "Data_mining :1.Numpy",
    "section": "np.ones",
    "text": "np.ones\nndarray를 만들 수 있는 넘파이 함수가 많습니다.\n다음은 1로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다:\nnp.ones((3,4))\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.full",
    "href": "posts/first_numpy/first_lect.html#np.full",
    "title": "Data_mining :1.Numpy",
    "section": "np.full",
    "text": "np.full\n주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 \\(3 \\times 4\\) 크기의 행렬입니다.\nnp.full((3,4), np.pi)\narray([[3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265],\n       [3.14159265, 3.14159265, 3.14159265, 3.14159265]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.empty",
    "href": "posts/first_numpy/first_lect.html#np.empty",
    "title": "Data_mining :1.Numpy",
    "section": "np.empty",
    "text": "np.empty\n초기화되지 않은 \\(2 \\times 3\\) 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다):\nnp.empty((2,3))\narray([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000],\n       [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.array",
    "href": "posts/first_numpy/first_lect.html#np.array",
    "title": "Data_mining :1.Numpy",
    "section": "np.array",
    "text": "np.array\narray 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다:\nnp.array([[1,2,3,4], [10, 20, 30, 40]])\narray([[ 1,  2,  3,  4],\n       [10, 20, 30, 40]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.arange",
    "href": "posts/first_numpy/first_lect.html#np.arange",
    "title": "Data_mining :1.Numpy",
    "section": "np.arange",
    "text": "np.arange\n파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다:\nnp.arange(1, 5)\narray([1, 2, 3, 4])\n부동 소수도 가능합니다:\nnp.arange(1.0, 5.0)\narray([1., 2., 3., 4.])\n파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다:\nnp.arange(1, 5, 0.5)\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5])\n부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다:\nprint(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다.\nprint(np.arange(0, 5/3, 0.333333333))\nprint(np.arange(0, 5/3, 0.333333334))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]\n[0.         0.33333333 0.66666667 1.         1.33333334]\nfor loops를 사용하지 않고 전체 array에 대한 연산 수행이 가능합니다.\n평균적으로 Numpy-based 알고리즘은 10~100배정도 속도가 더 빠르고 적은 메모리를 사용합니다.\nmy_arr = np.arange(1000000)\nmy_list = list(range(1000000))\n\n%time for _ in range(10): my_arr2 = my_arr * 2\n%time for _ in range(10): my_list2 = [x * 2 for x in my_list]\nCPU times: user 10.6 ms, sys: 24.1 ms, total: 34.7 ms\nWall time: 38.3 ms\nCPU times: user 705 ms, sys: 324 ms, total: 1.03 s\nWall time: 1.04 s\nFor loop를 돌릴 때의 속도 비교\nimport sys\n\nsize = 10\n\n%timeit for x in range(size): x ** 2\n# out: 10 loops, best of 3: 136 ms per loop\n\n# avoid this\n%timeit for x in np.arange(size): x ** 2\n#out: 1 loops, best of 3: 1.16 s per loop\n\n# use this\n%timeit np.arange(size) ** 2\n#out: 100 loops, best of 3: 19.5 ms per loop\n\n##loop문 사용할 때는 np 사용 지양하기\n2.49 µs ± 46.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\nThe slowest run took 4.95 times longer than the fastest. This could mean that an intermediate result is being cached.\n9.92 µs ± 4.81 µs per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.24 µs ± 859 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\nnp.arange(size) ** 2\narray([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.linspace",
    "href": "posts/first_numpy/first_lect.html#np.linspace",
    "title": "Data_mining :1.Numpy",
    "section": "np.linspace",
    "text": "np.linspace\n이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다):\nprint(np.linspace(0, 5/3, 6))\n[0.         0.33333333 0.66666667 1.         1.33333333 1.66666667]"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.rand와-np.randn",
    "href": "posts/first_numpy/first_lect.html#np.rand와-np.randn",
    "title": "Data_mining :1.Numpy",
    "section": "np.rand와 np.randn",
    "text": "np.rand와 np.randn\n넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 \\(3 \\times 4\\) 행렬을 초기화합니다:\nnp.random.rand(3,4)\narray([[0.37892456, 0.17966937, 0.38206837, 0.34922123],\n       [0.80462136, 0.9845914 , 0.9416127 , 0.28305275],\n       [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]])\n다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 \\(3 \\times 4\\) 행렬입니다:\nnp.random.randn(3,4)\narray([[ 0.83811287, -0.57131751, -0.4381827 ,  1.1485899 ],\n       [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ],\n       [ 1.01003549,  1.04381736, -0.93060038,  2.39043293]])\n이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요):\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.hist(np.random.rand(100000), density=True, bins=100, histtype=\"step\", color=\"blue\", label=\"rand\")\nplt.hist(np.random.randn(100000), density=True, bins=100, histtype=\"step\", color=\"red\", label=\"randn\")\nplt.axis([-2.5, 2.5, 0, 1.1])\nplt.legend(loc = \"upper left\")\nplt.title(\"Random distributions\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Density\")\nplt.show()\n\n\n\npng"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.fromfunction",
    "href": "posts/first_numpy/first_lect.html#np.fromfunction",
    "title": "Data_mining :1.Numpy",
    "section": "np.fromfunction",
    "text": "np.fromfunction\n함수를 사용하여 ndarray를 초기화할 수도 있습니다:\ndef my_function(z, y, x):\n    return x + 10 * y + 100 * z\n\nnp.fromfunction(my_function, (3, 2, 10))\narray([[[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.],\n        [ 10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.]],\n\n       [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.],\n        [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]],\n\n       [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.],\n        [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]])\n넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다:\n[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n\n [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n  [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n\n [[ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]\n  [ 2.  2.  2.  2.  2.  2.  2.  2.  2.  2.]]]\n위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#dtype",
    "href": "posts/first_numpy/first_lect.html#dtype",
    "title": "Data_mining :1.Numpy",
    "section": "dtype",
    "text": "dtype\n넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다:\nc = np.arange(1, 5)\nprint(c.dtype, c)\nint32 [1 2 3 4]\nc = np.arange(1.0, 5.0)\nprint(c.dtype, c)\nfloat64 [1. 2. 3. 4.]\n넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다:\nd = np.arange(1, 5, dtype=np.complex64)\nprint(d.dtype, d)\ncomplex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j]\n가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#itemsize",
    "href": "posts/first_numpy/first_lect.html#itemsize",
    "title": "Data_mining :1.Numpy",
    "section": "itemsize",
    "text": "itemsize\nitemsize 속성은 각 아이템의 크기(바이트)를 반환합니다:\ne = np.arange(1, 5, dtype=np.complex64)\ne.itemsize\n8"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#data-버퍼",
    "href": "posts/first_numpy/first_lect.html#data-버퍼",
    "title": "Data_mining :1.Numpy",
    "section": "data 버퍼",
    "text": "data 버퍼\n배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요).\nf = np.array([[1,2],[1000, 2000]], dtype=np.int32)\nf.data\n<memory at 0x7f97929dd790>\n파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다.\nif (hasattr(f.data, \"tobytes\")):\n    data_bytes = f.data.tobytes() # python 3\nelse:\n    data_bytes = memoryview(f.data).tobytes() # python 2\n\ndata_bytes\nb'\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\xe8\\x03\\x00\\x00\\xd0\\x07\\x00\\x00'\n여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#자신을-변경",
    "href": "posts/first_numpy/first_lect.html#자신을-변경",
    "title": "Data_mining :1.Numpy",
    "section": "자신을 변경",
    "text": "자신을 변경\nndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다.\ng = np.arange(24)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n랭크: 1\ng.shape = (6, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]\n [12 13 14 15]\n [16 17 18 19]\n [20 21 22 23]]\n랭크: 2\ng.shape = (2, 3, 4)\nprint(g)\nprint(\"랭크:\", g.ndim)\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n랭크: 3\ng[1,1,1]\n17"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#reshape",
    "href": "posts/first_numpy/first_lect.html#reshape",
    "title": "Data_mining :1.Numpy",
    "section": "reshape",
    "text": "reshape\nreshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다.\ng2 = g.reshape(4,6)\nprint(g2)\nprint(\"랭크:\", g2.ndim)\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]]\n랭크: 2\ng[0,0,0] = 10\ng2\narray([[10,  1,  2,  3,  4,  5],\n       [ 6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17],\n       [18, 19, 20, 21, 22, 23]])\n행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요).\ng2[1, 2] = 999\ng2\narray([[ 10,   1,   2,   3,   4,   5],\n       [  6,   7, 999,   9,  10,  11],\n       [ 12,  13,  14,  15,  16,  17],\n       [ 18,  19,  20,  21,  22,  23]])\n이에 상응하는 g의 원소도 수정됩니다.\ng\narray([[[ 10,   1,   2,   3],\n        [  4,   5,   6,   7],\n        [999,   9,  10,  11]],\n\n       [[ 12,  13,  14,  15],\n        [ 16,  17,  18,  19],\n        [ 20,  21,  22,  23]]])\n완전히 다른 공간에 값만 같게 복사를 하고 싶다면 copy를 사용.\n이렇게 할 경우 두 객체는 독립적인 객체로 존재함\ng2 = g.copy()"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#ravel",
    "href": "posts/first_numpy/first_lect.html#ravel",
    "title": "Data_mining :1.Numpy",
    "section": "ravel",
    "text": "ravel\n마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다:\ng.ravel()\narray([ 10,   1,   2,   3,   4,   5,   6,   7, 999,   9,  10,  11,  12,\n        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#규칙-1",
    "href": "posts/first_numpy/first_lect.html#규칙-1",
    "title": "Data_mining :1.Numpy",
    "section": "규칙 1",
    "text": "규칙 1\n배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다.\nh = np.arange(5).reshape(1, 1, 5)\nh\narray([[[0, 1, 2, 3, 4]]])\n여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다!\nh + [10, 20, 30, 40, 50]  # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]]\narray([[[10, 21, 32, 43, 54]]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#규칙-2",
    "href": "posts/first_numpy/first_lect.html#규칙-2",
    "title": "Data_mining :1.Numpy",
    "section": "규칙 2",
    "text": "규칙 2\n특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다.\nk = np.arange(6).reshape(2, 3)\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\n(2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다:\nk + [[100], [200]]  # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]]\narray([[100, 101, 102],\n       [203, 204, 205]])\n규칙 1과 2를 합치면 다음과 같이 동작합니다:\n(2,3) 크기의 ndarray에 (3,) 크기의 ndarray 더하기\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\nk + [100, 200, 300]  # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]]\narray([[100, 201, 302],\n       [103, 204, 305]])\ntest = np.array([100, 200, 300])\ntest.shape\ntest\narray([100, 200, 300])\n# step 1\ntest = test.reshape(1,3)\ntest\narray([[100, 200, 300]])\n# step 2\nnp.vstack((test,test))\narray([[100, 200, 300],\n       [100, 200, 300]])\n# step 2\nnp.concatenate((test,test),axis=0)\narray([[100, 200, 300],\n       [100, 200, 300]])\n또 매우 간단히 다음 처럼 해도 됩니다:\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\nk + 1000  # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]]\narray([[1000, 1001, 1002],\n       [1003, 1004, 1005]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#규칙-3",
    "href": "posts/first_numpy/first_lect.html#규칙-3",
    "title": "Data_mining :1.Numpy",
    "section": "규칙 3",
    "text": "규칙 3\n규칙 1 & 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다.\nk\narray([[0, 1, 2],\n       [3, 4, 5]])\ntry:\n    k + [33, 44]\nexcept ValueError as e:\n    print(e)\noperands could not be broadcast together with shapes (2,3) (2,) \n브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요.\na = np.array([[0.0],[10.0],[20.0],[30.0]])\na\narray([[ 0.],\n       [10.],\n       [20.],\n       [30.]])\na = np.array([0.0, 10.0, 20.0, 30.0])\nb = np.array([1.0, 2.0, 3.0])\na[:, np.newaxis] + b\narray([[ 1.,  2.,  3.],\n       [11., 12., 13.],\n       [21., 22., 23.],\n       [31., 32., 33.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#업캐스팅",
    "href": "posts/first_numpy/first_lect.html#업캐스팅",
    "title": "Data_mining :1.Numpy",
    "section": "업캐스팅",
    "text": "업캐스팅\ndtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다.\nk1 = np.arange(0, 5, dtype=np.uint8)\nprint(k1.dtype, k1)\nuint8 [0 1 2 3 4]\nk2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8)\nprint(k2.dtype, k2)\nint16 [ 5  7  9 11 13]\n모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다.\nk3 = k1 + 1.5\nprint(k3.dtype, k3)\nfloat64 [1.5 2.5 3.5 4.5 5.5]"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#ndarray-메서드",
    "href": "posts/first_numpy/first_lect.html#ndarray-메서드",
    "title": "Data_mining :1.Numpy",
    "section": "ndarray 메서드",
    "text": "ndarray 메서드\n일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean(axis = 0))\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = [3.75 7.05 9.5 ]\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nprint(a)\nprint(\"평균 =\", a.mean(axis = 1))\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n평균 = [ 2.53333333 11.        ]\n이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다.\n다음은 유용한 ndarray 메서드입니다:\nfor func in (a.min, a.max, a.sum, a.prod, a.std, a.var):\n    print(func.__name__, \"=\", func())\nmin = -2.5\nmax = 12.0\nsum = 40.6\nprod = -71610.0\nstd = 5.084835843520964\nvar = 25.855555555555554\n이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면:\nc=np.arange(24).reshape(2,3,4)\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#section",
    "href": "posts/first_numpy/first_lect.html#section",
    "title": "Data_mining :1.Numpy",
    "section": "!",
    "text": "!\nc.sum(axis=0)  # 첫 번째 축을 따라 더함, 결과는 3x4 배열\narray([[12, 14, 16, 18],\n       [20, 22, 24, 26],\n       [28, 30, 32, 34]])\nc.sum(axis=1)  # 두 번째 축을 따라 더함, 결과는 2x4 배열\narray([[12, 15, 18, 21],\n       [48, 51, 54, 57]])\nc.sum(axis = 1).shape\n(2, 4)\nc.sum(axis=2) \narray([[ 6, 22, 38],\n       [54, 70, 86]])\n여러 축에 대해서 더할 수도 있습니다:\nc\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]])\nc.sum(axis=(0,2))  # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열\narray([ 60,  92, 124])\n0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23\n(60, 92, 124)"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#quiz-아래의-array를-사용해서-다음-퀴즈를-풀어봅시다.",
    "href": "posts/first_numpy/first_lect.html#quiz-아래의-array를-사용해서-다음-퀴즈를-풀어봅시다.",
    "title": "Data_mining :1.Numpy",
    "section": "Quiz 아래의 array를 사용해서 다음 퀴즈를 풀어봅시다.",
    "text": "Quiz 아래의 array를 사용해서 다음 퀴즈를 풀어봅시다.\narray_2d = np.array([[5,10,15],\n                     [20,25,30],\n                     [35,40,45]])\n\n1. 2차원 배열 array_2d에서 첫 번째 행(row)의 모든 요소를 선택해 보세요.\n\n힌트:인덱싱을 사용하여 첫 번째 행을 선택할 수 있음\narray_2d[0]\narray([ 5, 10, 15])\n\n\n\n2. 두 번째 열의 모든 요소를 선택해 보세요.\n힌트 : 인덱싱과 슬라이싱을 사용하여 두 번째 열 선택 가능\narray_2d[:,1]\narray([10, 25, 40])\n\n\n3. 다음 요소들을 선택해 보세요 25,30,40,45\n\n4. 팬시 인덱싱을 사용\narray_2d[1:,1:]\narray([[25, 30],\n       [40, 45]])\narray_2d[(1,2),1:3]\narray([[25, 30],\n       [40, 45]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#일반-함수",
    "href": "posts/first_numpy/first_lect.html#일반-함수",
    "title": "Data_mining :1.Numpy",
    "section": "일반 함수",
    "text": "일반 함수\n넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다:\na = np.array([[-2.5, 3.1, 7], [10, 11, 12]])\nnp.square(a)\narray([[  6.25,   9.61,  49.  ],\n       [100.  , 121.  , 144.  ]])\n다음은 유용한 단항 일반 함수들입니다:\nprint(\"원본 ndarray\")\nprint(a)\nfor func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos):\n    print(\"\\n\", func.__name__)\n    print(func(a))\n원본 ndarray\n[[-2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n absolute\n[[ 2.5  3.1  7. ]\n [10.  11.  12. ]]\n\n sqrt\n[[       nan 1.76068169 2.64575131]\n [3.16227766 3.31662479 3.46410162]]\n\n exp\n[[8.20849986e-02 2.21979513e+01 1.09663316e+03]\n [2.20264658e+04 5.98741417e+04 1.62754791e+05]]\n\n log\n[[       nan 1.13140211 1.94591015]\n [2.30258509 2.39789527 2.48490665]]\n\n sign\n[[-1.  1.  1.]\n [ 1.  1.  1.]]\n\n ceil\n[[-2.  4.  7.]\n [10. 11. 12.]]\n\n modf\n(array([[-0.5,  0.1,  0. ],\n       [ 0. ,  0. ,  0. ]]), array([[-2.,  3.,  7.],\n       [10., 11., 12.]]))\n\n isnan\n[[False False False]\n [False False False]]\n\n cos\n[[-0.80114362 -0.99913515  0.75390225]\n [-0.83907153  0.0044257   0.84385396]]\n\n\nC:\\Users\\user\\AppData\\Local\\Temp/ipykernel_10176/3372443465.py:5: RuntimeWarning: invalid value encountered in sqrt\n  print(func(a))\nC:\\Users\\user\\AppData\\Local\\Temp/ipykernel_10176/3372443465.py:5: RuntimeWarning: invalid value encountered in log\n  print(func(a))"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#이항-일반-함수",
    "href": "posts/first_numpy/first_lect.html#이항-일반-함수",
    "title": "Data_mining :1.Numpy",
    "section": "이항 일반 함수",
    "text": "이항 일반 함수\n두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다:\na = np.array([1, -2, 3, 4])\nb = np.array([2, 8, -1, 7])\nnp.add(a, b)  # a + b 와 동일\narray([ 3,  6,  2, 11])\nnp.greater(a, b)  # a > b 와 동일\narray([False, False,  True, False])\nnp.maximum(a, b)\narray([2, 8, 3, 7])\nnp.copysign(a, b)\narray([ 1.,  2., -3.,  4.])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#차원-배열",
    "href": "posts/first_numpy/first_lect.html#차원-배열",
    "title": "Data_mining :1.Numpy",
    "section": "1차원 배열",
    "text": "1차원 배열\n1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다:\na = np.array([1, 5, 3, 19, 13, 7, 3])\na[3]\n19\na[2:5]\narray([ 3, 19, 13])\na[2:-1]\narray([ 3, 19, 13,  7])\na[:2]\narray([1, 5])\na[2::2]\narray([ 3, 13,  3])\na[2::3]\narray([], shape=(0, 3), dtype=float64)\na[::-1]\narray([ 3,  7, 13, 19,  3,  5,  1])\n물론 원소를 수정할 수 있죠:\na[3]=999\na\narray([  1,   5,   3, 999,  13,   7,   3])\n슬라이싱을 사용해 ndarray를 수정할 수 있습니다:\na[2:5] = [997, 998, 999]\na\narray([  1,   5, 997, 998, 999,   7,   3])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#보통의-파이썬-배열과-차이점",
    "href": "posts/first_numpy/first_lect.html#보통의-파이썬-배열과-차이점",
    "title": "Data_mining :1.Numpy",
    "section": "보통의 파이썬 배열과 차이점",
    "text": "보통의 파이썬 배열과 차이점\n보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다.\na[2:5] = -1\na\narray([ 1,  5, -1, -1, -1,  7,  3])\nList는 브로드캐스팅으로 할당이 안됨\nb = [1, 5, 3, 19, 13, 7, 3]\nb[2:5] = -1\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n~\\AppData\\Local\\Temp/ipykernel_10176/1794665673.py in <module>\n      1 b = [1, 5, 3, 19, 13, 7, 3]\n----> 2 b[2:5] = -1\n\n\nTypeError: can only assign an iterable\n또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다:\ntry:\n    a[2:5] = [1,2,3,4,5,6]  # 너무 길어요\nexcept ValueError as e:\n    print(e)\ncould not broadcast input array from shape (6,) into shape (3,)\n원소를 삭제할 수도 없습니다:\ntry:\n    del a[2:5]\nexcept ValueError as e:\n    print(e)\ncannot delete array elements\nList에서는 삭제가 가능\nb = [1, 5, 3, 19, 13, 7, 3] \ndel b[2:5]\nb\n[1, 5, 7, 3]\n중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다!\na_slice = a[2:6]\na_slice[1] = 1000\na  # 원본 배열이 수정됩니다!\narray([   1,    5,    1, 1000,    3,    7,    3])\na[3] = 2000\na_slice  # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다!\narray([   1, 2000,    3,    7])\n데이터를 복사하려면 copy 메서드를 사용해야 합니다:\nanother_slice = a[2:6].copy()\nanother_slice[1] = 3000\na  # 원본 배열이 수정되지 않습니다\narray([   1,    5,    1, 2000,    3,    7,    3])\na[3] = 4000\nanother_slice  # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다\narray([   1, 3000,    3,    7])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#다차원-배열",
    "href": "posts/first_numpy/first_lect.html#다차원-배열",
    "title": "Data_mining :1.Numpy",
    "section": "다차원 배열",
    "text": "다차원 배열\n다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다:\nimport numpy as np\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[1, 2]  # 행 1, 열 2\n14\nb[1, :]  # 행 1, 모든 열\narray([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\nb[:, 1]  # 모든 행, 열 1\narray([ 1, 13, 25, 37])\n주의: 다음 두 표현에는 미묘한 차이가 있습니다:\nb[1, :].shape\n(12,)\nb[1:,]\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[1:2, :]\narray([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]])\n첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#팬시-인덱싱fancy-indexing",
    "href": "posts/first_numpy/first_lect.html#팬시-인덱싱fancy-indexing",
    "title": "Data_mining :1.Numpy",
    "section": "팬시 인덱싱(Fancy indexing)",
    "text": "팬시 인덱싱(Fancy indexing)\n관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다.\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nb[(0,2), 2:5]  # 행 0과 2, 열 2에서 4(5-1)까지\narray([[ 2,  3,  4],\n       [26, 27, 28]])\nb[:, (-1, 2, -1)]  # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로)\narray([[11,  2, 11],\n       [23, 14, 23],\n       [35, 26, 35],\n       [47, 38, 47]])\n여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다.\nb[(-1, 2, -1, 2), (5, 9, 1, 9)]  # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again)\narray([41, 33, 37, 33])\nb[(2,3),0:2]\narray([[24, 25],\n       [36, 37]])\n#위와 같은 결과\nb[2:,0:2]\narray([[24, 25],\n       [36, 37]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#고차원",
    "href": "posts/first_numpy/first_lect.html#고차원",
    "title": "Data_mining :1.Numpy",
    "section": "고차원",
    "text": "고차원\n고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다:\nc = b.reshape(4,2,6)\nc\narray([[[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]],\n\n       [[12, 13, 14, 15, 16, 17],\n        [18, 19, 20, 21, 22, 23]],\n\n       [[24, 25, 26, 27, 28, 29],\n        [30, 31, 32, 33, 34, 35]],\n\n       [[36, 37, 38, 39, 40, 41],\n        [42, 43, 44, 45, 46, 47]]])\nc[2, 1, 4]  # 행렬 2, 행 1, 열 4\n34\nc[2, :, 3]  # 행렬 2, 모든 행, 열 3\narray([27, 33])\n어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다:\nc[2, 1]  # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다.\narray([30, 31, 32, 33, 34, 35])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#생략-부호-...",
    "href": "posts/first_numpy/first_lect.html#생략-부호-...",
    "title": "Data_mining :1.Numpy",
    "section": "생략 부호 (...)",
    "text": "생략 부호 (...)\n생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다.\nc[2, ...]  #  행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일\narray([[24, 25, 26, 27, 28, 29],\n       [30, 31, 32, 33, 34, 35]])\nc[2, 1, ...]  # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일\narray([30, 31, 32, 33, 34, 35])\nc[2, ..., 3]  # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일\narray([27, 33])\nc[..., 3]  # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일\narray([[ 3,  9],\n       [15, 21],\n       [27, 33],\n       [39, 45]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#불리언-인덱싱",
    "href": "posts/first_numpy/first_lect.html#불리언-인덱싱",
    "title": "Data_mining :1.Numpy",
    "section": "불리언 인덱싱",
    "text": "불리언 인덱싱\n불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다.\nb = np.arange(48).reshape(4, 12)\nb\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n       [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\nrows_on = np.array([True, False, True, False])\nb[rows_on, :]  # 행 0과 2, 모든 열. b[(0, 2), :]와 동일\narray([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n       [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]])\ncols_on = np.array([False, True, False] * 4)\nb[:, cols_on]  # 모든 행, 열 1, 4, 7, 10\narray([[ 1,  4,  7, 10],\n       [13, 16, 19, 22],\n       [25, 28, 31, 34],\n       [37, 40, 43, 46]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#np.ix_",
    "href": "posts/first_numpy/first_lect.html#np.ix_",
    "title": "Data_mining :1.Numpy",
    "section": "np.ix_",
    "text": "np.ix_\n여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다:\nb[np.ix_((0,2),(1,4,7,10))]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\nb[np.ix_(rows_on, cols_on)]\narray([[ 1,  4,  7, 10],\n       [25, 28, 31, 34]])\nnp.ix_(rows_on, cols_on)\n(array([[0],\n        [2]], dtype=int64),\n array([[ 1,  4,  7, 10]], dtype=int64))\nndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다:\nb.shape\n(4, 12)\nb[b % 3 == 1]\narray([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#vstack",
    "href": "posts/first_numpy/first_lect.html#vstack",
    "title": "Data_mining :1.Numpy",
    "section": "vstack",
    "text": "vstack\nvstack 함수를 사용하여 수직으로 쌓아보죠:\nq4 = np.vstack((q1, q2, q3))\nq4\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq4.shape\n(10, 4)\nq1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다)."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#hstack",
    "href": "posts/first_numpy/first_lect.html#hstack",
    "title": "Data_mining :1.Numpy",
    "section": "hstack",
    "text": "hstack\nhstack을 사용해 수평으로도 쌓을 수 있습니다:\nq5 = np.hstack((q1, q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\nq5.shape\n(3, 8)\nq1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다:\ntry:\n    q5 = np.hstack((q1, q2, q3))\nexcept ValueError as e:\n    print(e)\nall the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#concatenate",
    "href": "posts/first_numpy/first_lect.html#concatenate",
    "title": "Data_mining :1.Numpy",
    "section": "concatenate",
    "text": "concatenate\nconcatenate 함수는 지정한 축으로도 배열을 쌓습니다.\nq7 = np.concatenate((q1, q2, q3), axis=0)  # vstack과 동일\nq7\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [2., 2., 2., 2.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.],\n       [3., 3., 3., 3.]])\nq7.shape\n(10, 4)\nq5 = np.hstack((q1,q3))\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\nq5 = np.concatenate((q1,q3), axis = 1)\nq5\narray([[1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.],\n       [1., 1., 1., 1., 3., 3., 3., 3.]])\n예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#stack",
    "href": "posts/first_numpy/first_lect.html#stack",
    "title": "Data_mining :1.Numpy",
    "section": "stack",
    "text": "stack\nstack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다.\nq1.shape\n(3, 4)\nq3.shape\n(3, 4)\n# 형태는 그대로 유지\nq8 = np.stack((q1, q3))\nq8\narray([[[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [1., 1., 1., 1.]],\n\n       [[3., 3., 3., 3.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]]])\nq8.shape\n(2, 3, 4)"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#행렬-전치",
    "href": "posts/first_numpy/first_lect.html#행렬-전치",
    "title": "Data_mining :1.Numpy",
    "section": "행렬 전치",
    "text": "행렬 전치\nT 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다:\nm1 = np.arange(10).reshape(2,5)\nm1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nm1.T\narray([[0, 5],\n       [1, 6],\n       [2, 7],\n       [3, 8],\n       [4, 9]])\nT 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다:\nm2 = np.arange(5)\nm2\narray([0, 1, 2, 3, 4])\nm2.T\narray([0, 1, 2, 3, 4])\n먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다:\nm2r = m2.reshape(1,5)\nm2r\narray([[0, 1, 2, 3, 4]])\nm2r.T\narray([[0],\n       [1],\n       [2],\n       [3],\n       [4]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#행렬-곱셈",
    "href": "posts/first_numpy/first_lect.html#행렬-곱셈",
    "title": "Data_mining :1.Numpy",
    "section": "행렬 곱셈",
    "text": "행렬 곱셈\n두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠.\nn1 = np.arange(10).reshape(2, 5)\nn1\narray([[0, 1, 2, 3, 4],\n       [5, 6, 7, 8, 9]])\nn2 = np.arange(15).reshape(5,3)\nn2\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11],\n       [12, 13, 14]])\nn1.dot(n2)\narray([[ 90, 100, 110],\n       [240, 275, 310]])\n주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다."
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#역행렬과-유사-역행렬",
    "href": "posts/first_numpy/first_lect.html#역행렬과-유사-역행렬",
    "title": "Data_mining :1.Numpy",
    "section": "역행렬과 유사 역행렬",
    "text": "역행렬과 유사 역행렬\nnumpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다:\nimport numpy.linalg as linalg\n\nm3 = np.array([[1,2,3],[5,7,11],[21,29,31]])\nm3\narray([[ 1,  2,  3],\n       [ 5,  7, 11],\n       [21, 29, 31]])\nlinalg.inv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])\npinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다:\nlinalg.pinv(m3)\narray([[-2.31818182,  0.56818182,  0.02272727],\n       [ 1.72727273, -0.72727273,  0.09090909],\n       [-0.04545455,  0.29545455, -0.06818182]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#단위-행렬",
    "href": "posts/first_numpy/first_lect.html#단위-행렬",
    "title": "Data_mining :1.Numpy",
    "section": "단위 행렬",
    "text": "단위 행렬\n행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다):\nm3.dot(linalg.inv(m3))\narray([[ 1.00000000e+00, -1.66533454e-16,  0.00000000e+00],\n       [ 6.31439345e-16,  1.00000000e+00, -1.38777878e-16],\n       [ 5.21110932e-15, -2.38697950e-15,  1.00000000e+00]])\neye 함수는 NxN 크기의 단위 행렬을 만듭니다:\nnp.eye(3)\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#qr-분해",
    "href": "posts/first_numpy/first_lect.html#qr-분해",
    "title": "Data_mining :1.Numpy",
    "section": "QR 분해",
    "text": "QR 분해\nqr 함수는 행렬을 QR 분해합니다:\nq, r = linalg.qr(m3)\nq\narray([[-0.04627448,  0.98786672,  0.14824986],\n       [-0.23137241,  0.13377362, -0.96362411],\n       [-0.97176411, -0.07889213,  0.22237479]])\nr\narray([[-21.61018278, -29.89331494, -32.80860727],\n       [  0.        ,   0.62427688,   1.9894538 ],\n       [  0.        ,   0.        ,  -3.26149699]])\nq.dot(r)  # q.r는 m3와 같습니다\narray([[ 1.,  2.,  3.],\n       [ 5.,  7., 11.],\n       [21., 29., 31.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#행렬식",
    "href": "posts/first_numpy/first_lect.html#행렬식",
    "title": "Data_mining :1.Numpy",
    "section": "행렬식",
    "text": "행렬식\ndet 함수는 행렬식을 계산합니다:\nlinalg.det(m3)  # 행렬식 계산\n43.99999999999997"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#고윳값과-고유벡터",
    "href": "posts/first_numpy/first_lect.html#고윳값과-고유벡터",
    "title": "Data_mining :1.Numpy",
    "section": "고윳값과 고유벡터",
    "text": "고윳값과 고유벡터\neig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다:\neigenvalues, eigenvectors = linalg.eig(m3)\neigenvalues # λ\narray([42.26600592, -0.35798416, -2.90802176])\neigenvectors # v\narray([[-0.08381182, -0.76283526, -0.18913107],\n       [-0.3075286 ,  0.64133975, -0.6853186 ],\n       [-0.94784057, -0.08225377,  0.70325518]])\nm3.dot(eigenvectors) - eigenvalues * eigenvectors  # m3.v - λ*v = 0\narray([[ 8.88178420e-15,  2.22044605e-16, -3.10862447e-15],\n       [ 3.55271368e-15,  2.02615702e-15, -1.11022302e-15],\n       [ 3.55271368e-14,  3.33413852e-15, -8.43769499e-15]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#특잇값-분해",
    "href": "posts/first_numpy/first_lect.html#특잇값-분해",
    "title": "Data_mining :1.Numpy",
    "section": "특잇값 분해",
    "text": "특잇값 분해\nsvd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다:\nm4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]])\nm4\narray([[1, 0, 0, 0, 2],\n       [0, 0, 3, 0, 0],\n       [0, 0, 0, 0, 0],\n       [0, 2, 0, 0, 0]])\nU, S_diag, V = linalg.svd(m4)\nU\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0., -1.],\n       [ 0.,  0.,  1.,  0.]])\nS_diag\narray([3.        , 2.23606798, 2.        , 0.        ])\nsvd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다:\nS = np.zeros((4, 5))\nS[np.diag_indices(4)] = S_diag\nS  # Σ\narray([[3.        , 0.        , 0.        , 0.        , 0.        ],\n       [0.        , 2.23606798, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 2.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        , 0.        ]])\nV\narray([[-0.        ,  0.        ,  1.        , -0.        ,  0.        ],\n       [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n       [-0.        ,  1.        ,  0.        , -0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n       [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]])\nU.dot(S).dot(V) # U.Σ.V == m4\narray([[1., 0., 0., 0., 2.],\n       [0., 0., 3., 0., 0.],\n       [0., 0., 0., 0., 0.],\n       [0., 2., 0., 0., 0.]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#대각원소와-대각합",
    "href": "posts/first_numpy/first_lect.html#대각원소와-대각합",
    "title": "Data_mining :1.Numpy",
    "section": "대각원소와 대각합",
    "text": "대각원소와 대각합\nnp.diag(m3)  # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래)\narray([ 1,  7, 31])\nnp.trace(m3)  # np.diag(m3).sum()와 같습니다\n39"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#선형-방정식-풀기",
    "href": "posts/first_numpy/first_lect.html#선형-방정식-풀기",
    "title": "Data_mining :1.Numpy",
    "section": "선형 방정식 풀기",
    "text": "선형 방정식 풀기\nsolve 함수는 다음과 같은 선형 방정식을 풉니다:\n\n\\(2x + 6y = 6\\)\n\\(5x + 3y = -9\\)\n\ncoeffs  = np.array([[2, 6], [5, 3]])\ndepvars = np.array([6, -9])\nsolution = linalg.solve(coeffs, depvars)\nsolution\narray([-3.,  2.])\nsolution을 확인해 보죠:\ncoeffs.dot(solution), depvars  # 네 같네요\n(array([ 6., -9.]), array([ 6, -9]))\n좋습니다! 다른 방식으로도 solution을 확인해 보죠:\nnp.allclose(coeffs.dot(solution), depvars)\nTrue"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#바이너리-.npy-포맷",
    "href": "posts/first_numpy/first_lect.html#바이너리-.npy-포맷",
    "title": "Data_mining :1.Numpy",
    "section": "바이너리 .npy 포맷",
    "text": "바이너리 .npy 포맷\n랜덤 배열을 만들고 저장해 보죠.\na = np.random.rand(2,3)\na\narray([[0.9547208 , 0.57325645, 0.33391344],\n       [0.98385576, 0.78616947, 0.75662365]])\nnp.save(\"my_array\", a)\n끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.npy\", \"rb\") as f:\n    content = f.read()\n\ncontent\nb\"\\x93NUMPY\\x01\\x00v\\x00{'descr': '<f8', 'fortran_order': False, 'shape': (2, 3), }                                                          \\nl\\x97\\xc27\\x185\\xd0?\\x8d\\x07\\xd9Q\\x12\\xf8\\xe5?\\x17\\xde\\x18\\xfcZq\\xea?\\xa3\\x91\\xe1q-3\\xe8?\\xeeASq\\xc8\\x87\\xec?\\x8f\\xb5\\x06r\\x01\\xd6\\xe2?\"\n이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다:\na_loaded = np.load(\"my_array.npy\")\na_loaded\narray([[0.25324064, 0.68653217, 0.82633733],\n       [0.75624726, 0.89157507, 0.58862374]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#텍스트-포맷",
    "href": "posts/first_numpy/first_lect.html#텍스트-포맷",
    "title": "Data_mining :1.Numpy",
    "section": "텍스트 포맷",
    "text": "텍스트 포맷\n배열을 텍스트 포맷으로 저장해 보죠:\nnp.savetxt(\"my_array.csv\", a)\n파일 내용을 확인해 보겠습니다:\nwith open(\"my_array.csv\", \"rt\") as f:\n    print(f.read())\n2.532406372578097642e-01 6.865321730222021523e-01 8.263373302242510432e-01\n7.562472557297507114e-01 8.915750707038208045e-01 5.886237361025211667e-01\n이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다:\nnp.savetxt(\"my_array.csv\", a, delimiter=\",\")\n이 파일을 로드하려면 loadtxt 함수를 사용합니다:\na_loaded = np.loadtxt(\"my_array.csv\", delimiter=\",\")\na_loaded\narray([[0.25324064, 0.68653217, 0.82633733],\n       [0.75624726, 0.89157507, 0.58862374]])"
  },
  {
    "objectID": "posts/first_numpy/first_lect.html#압축된-.npz-포맷",
    "href": "posts/first_numpy/first_lect.html#압축된-.npz-포맷",
    "title": "Data_mining :1.Numpy",
    "section": "압축된 .npz 포맷",
    "text": "압축된 .npz 포맷\n여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다:\nb = np.arange(24, dtype=np.uint8).reshape(2, 3, 4)\nb\narray([[[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]],\n\n       [[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]]], dtype=uint8)\nnp.savez(\"my_arrays\", my_a=a, my_b=b)\n파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다.\nwith open(\"my_arrays.npz\", \"rb\") as f:\n    content = f.read()\n\nrepr(content)[:180] + \"[...]\"\n'b\"PK\\\\x03\\\\x04\\\\x14\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00!\\\\x007\\\\x16\\\\xcb\\\\xb7\\\\xb0\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x08\\\\x00\\\\x14\\\\x00my_a.npy\\\\x01\\\\x00\\\\x10\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\x00\\\\xb0\\\\x00\\\\x00\\\\x[...]'\n다음과 같이 이 파일을 로드할 수 있습니다:\nmy_arrays = np.load(\"my_arrays.npz\")\nmy_arrays\n<numpy.lib.npyio.NpzFile at 0x187e4f94df0>\n게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다:\nlist(my_arrays.keys())\n['my_a', 'my_b']\nmy_arrays[\"my_a\"]\narray([[0.15661405, 0.58827297, 0.04122154],\n       [0.55722113, 0.62681091, 0.58300341]])"
  },
  {
    "objectID": "posts/new_post/index.html",
    "href": "posts/new_post/index.html",
    "title": "New post",
    "section": "",
    "text": "블로그에 자유롭게 글을 쓰는 것이 가능합니다.\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1000개의 랜덤한 점 생성\nx = np.random.rand(1000)\ny = np.random.rand(1000)\n\n# scatterplot 그리기\nplt.scatter(x, y, s=10, c='blue')\n\n# 축 라벨 지정\nplt.xlabel('X')\nplt.ylabel('Y')\n\n# 그래프 타이틀 지정\nplt.title('Scatter Plot')\n\n\n# 그래프 출력\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/quiz/exercise-coordinate-reference-systems.html",
    "href": "posts/quiz/exercise-coordinate-reference-systems.html",
    "title": "Quiz_2",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nIntroduction\nYou are a bird conservation expert and want to understand migration patterns of purple martins. In your research, you discover that these birds typically spend the summer breeding season in the eastern United States, and then migrate to South America for the winter. But since this bird is under threat of endangerment, you’d like to take a closer look at the locations that these birds are more likely to visit.\n\n\n\nThere are several protected areas in South America, which operate under special regulations to ensure that species that migrate (or live) there have the best opportunity to thrive. You’d like to know if purple martins tend to visit these areas. To answer this question, you’ll use some recently collected data that tracks the year-round location of eleven different birds.\nBefore you get started, run the code cell below to set everything up.\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex2 import *\n\n\n\nExercises\n\n1) Load the data.\nRun the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df.\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 different birds in the dataset.\n\n\n\n\n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n    \n  \n\n\n\n\nThere are 11 birds in the dataset, where each bird is identified by a unique value in the “tag-local-identifier” column. Each bird has several measurements, collected at different times of the year.\nUse the next code cell to create a GeoDataFrame birds.\n- birds should have all of the columns from birds_df, along with a “geometry” column that contains Point objects with (longitude, latitude) locations.\n- Set the CRS of birds to {'init': 'epsg:4326'}.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df['location-long'], birds_df['location-lat']))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init': 'epsg:4326'}\n\n# Check your answer\n#q_1.check()\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nNext, we load in the 'naturalearth_lowres' dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      16\n      11263077.0\n      North America\n      Haiti\n      HTI\n      14332\n      POLYGON ((-71.71236 19.71446, -71.62487 19.169...\n    \n  \n\n\n\n\nUse the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame.\nDon’t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don’t have to worry about color-coding the points to differentiate between birds, and you don’t have to differentiate starting points from ending points. We’ll do that in the next part of the exercise.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='white', linestyle=':', edgecolor='black')\nbirds.plot(markersize=1, ax=ax)\n\n# Uncomment to see a hint\n#q_2.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n##q_2.solution()\n\n\n\n3) Where does each bird start and end its journey? (Part 1)\nNow, we’re ready to look more closely at each bird’s path. Run the next code cell to create two GeoDataFrames: - path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. - start_gdf contains the starting points for each bird.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n\n\nUse the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird.\n- The format should be identical to that of start_gdf, with two columns (“tag-local-identifier” and “geometry”), where the “geometry” column contains Point objects. - Set the CRS of end_gdf to {'init': 'epsg:4326'}.\n\n# Your code here\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init': 'epsg:4326'}\n\n# Check your answer\n#q_3.check()\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Where does each bird start and end its journey? (Part 2)\nUse the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame.\n\n# Your code here\nax = americas.plot(figsize=(8,8), color='white', linestyle=':', edgecolor='black')\npath_gdf.plot( ax=ax, linestyle = \"-\")\nstart_gdf.plot(color = \"blue\", ax = ax,markersize=15)\nend_gdf.plot(markersize = 15, ax = ax, color = \"red\")\n\n# Uncomment to see a hint\n#q_4.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_4.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.solution()\n\n\n\n5) Where are the protected areas in South America? (Part 1)\nIt looks like all of the birds end up somewhere in South America. But are they going to protected areas?\nIn the next code cell, you’ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath.\n\n# Path of the shapefile to load\nprotected_filepath = r\"C:\\Users\\ENGLISH\\Desktop\\archive\\SAPA_Aug2019-shapefile\\SAPA_Aug2019-shapefile\\SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\nprotected_areas.head()\n\n# Check your answer\n#q_5.check()\n\n\n\n\n\n  \n    \n      \n      WDPAID\n      WDPA_PID\n      PA_DEF\n      NAME\n      ORIG_NAME\n      DESIG\n      DESIG_ENG\n      DESIG_TYPE\n      IUCN_CAT\n      INT_CRIT\n      ...\n      GOV_TYPE\n      OWN_TYPE\n      MANG_AUTH\n      MANG_PLAN\n      VERIF\n      METADATAID\n      SUB_LOC\n      PARENT_ISO\n      ISO3\n      geometry\n    \n  \n  \n    \n      0\n      14067.0\n      14067\n      1\n      Het Spaans Lagoen\n      Het Spaans Lagoen\n      Ramsar Site, Wetland of International Importance\n      Ramsar Site, Wetland of International Importance\n      International\n      Not Reported\n      Not Reported\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Management plan is not implemented and not ava...\n      State Verified\n      1856\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-69.97523 12.47379, -69.97523 12.473...\n    \n    \n      1\n      14003.0\n      14003\n      1\n      Bubali Pond Bird Sanctuary\n      Bubali Pond Bird Sanctuary\n      Bird Sanctuary\n      Bird Sanctuary\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Not Reported\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      POLYGON ((-70.04734 12.56329, -70.04615 12.563...\n    \n    \n      2\n      555624439.0\n      555624439\n      1\n      Arikok National Park\n      Arikok National Park\n      National Park\n      National Park\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Non-profit organisations\n      Non-profit organisations\n      Fundacion Parke Nacional Arikok\n      Not Reported\n      State Verified\n      1899\n      Not Reported\n      NLD\n      ABW\n      MULTIPOLYGON (((-69.96302 12.48384, -69.96295 ...\n    \n    \n      3\n      303894.0\n      303894\n      1\n      Madidi\n      Madidi\n      Area Natural de Manejo Integrado\n      Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-68.59060 -14.43388, -68.59062 -14.4...\n    \n    \n      4\n      303893.0\n      303893\n      1\n      Apolobamba\n      Apolobamba\n      Area Natural de Manejo Integado Nacional\n      National Natural Integrated Management Area\n      National\n      Not Reported\n      Not Applicable\n      ...\n      Federal or national ministry or agency\n      Not Reported\n      Not Reported\n      Not Reported\n      State Verified\n      1860\n      BO-L\n      BOL\n      BOL\n      POLYGON ((-69.20949 -14.73334, -69.20130 -14.7...\n    \n  \n\n5 rows × 29 columns\n\n\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where are the protected areas in South America? (Part 2)\nCreate a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You’ll notice that some protected areas are on land, while others are in marine waters.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize = (10,10), color = \"white\", edgecolor = \"black\")\nprotected_areas.plot(ax = ax, alpha= 0.2)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n7) What percentage of South America is protected?\nYou’re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds.\nAs a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the “REP_AREA” and “REP_M_AREA” columns, which contain the total area and total marine area, respectively, in square kilometers.\nRun the code cell below without changes.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nSouth America has 5396761.9116883585 square kilometers of protected areas.\n\n\nThen, to finish the calculation, you’ll use the south_america GeoDataFrame.\n\nsouth_america.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      20\n      3398.0\n      South America\n      Falkland Is.\n      FLK\n      282\n      POLYGON ((-61.20000 -51.85000, -60.00000 -51.2...\n    \n    \n      28\n      3461734.0\n      South America\n      Uruguay\n      URY\n      56045\n      POLYGON ((-57.62513 -30.21629, -56.97603 -30.1...\n    \n    \n      29\n      211049527.0\n      South America\n      Brazil\n      BRA\n      1839758\n      POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n    \n  \n\n\n\n\nCalculate the total area of South America by following these steps: - Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. - Convert your answer to have units of square kilometeters.\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg = 3035).area)/10**6\ntotalArea\n\n# Check your answer\n#q_7.check()\n\n17759005.81506123\n\n\n\n# Lines below will give you a hint or solution code\n#q_7.hint()\n#q_7.solution()\n\nRun the code cell below to calculate the percentage of South America that is protected.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\nApproximately 30.39% of South America is protected.\n\n\n\n\n8) Where are the birds in South America?\nSo, are the birds in protected areas?\nCreate a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America.\nTo exclude protected areas that are purely marine areas (with no land component), you can use the “MARINE” column (and plot only the rows in protected_areas[protected_areas['MARINE']!='2'], instead of every row in the protected_areas GeoDataFrame).\n\n# Your code here\nax = south_america.plot(figsize = (10,10), color = \"white\", edgecolor = \"black\")\nprotected_areas[protected_areas[\"MARINE\"]!='2'].plot(ax = ax, alpha = 0.2)\n\n# Uncomment to see a hint\n#q_8.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_8.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_8.solution()\n\n\n\n\nKeep going\nCreate stunning interactive maps with your geospatial data.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/quiz/exercise-interactive-maps.html",
    "href": "posts/quiz/exercise-interactive-maps.html",
    "title": "Quiz_3",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nIntroduction\nYou are an urban safety planner in Japan, and you are analyzing which areas of Japan need extra earthquake reinforcement. Which areas are both high in population density and prone to earthquakes?\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\nfrom folium import Choropleth, Circle, Marker\n\nWe define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved.\nThis function ensures that the maps are visible in all web browsers.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) Do earthquakes coincide with plate boundaries?\nRun the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The “coordinates” column is a list of (latitude, longitude) locations along the boundaries.\n\nplate_boundaries = gpd.read_file(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\Plate_Boundaries\\Plate_Boundaries\\Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\n\n\n\n\n  \n    \n      \n      HAZ_PLATES\n      HAZ_PLAT_1\n      HAZ_PLAT_2\n      Shape_Leng\n      coordinates\n    \n  \n  \n    \n      0\n      TRENCH\n      SERAM TROUGH (ACTIVE)\n      6722\n      5.843467\n      [(-5.444200361999947, 133.6808931800001), (-5....\n    \n    \n      1\n      TRENCH\n      WETAR THRUST\n      6722\n      1.829013\n      [(-7.760600482999962, 125.47879802900002), (-7...\n    \n    \n      2\n      TRENCH\n      TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ...\n      6621\n      6.743604\n      [(19.817899819000047, 120.09999798800004), (19...\n    \n    \n      3\n      TRENCH\n      BONIN TRENCH\n      9821\n      8.329381\n      [(26.175899215000072, 143.20620700100005), (26...\n    \n    \n      4\n      TRENCH\n      NEW GUINEA TRENCH\n      8001\n      11.998145\n      [(0.41880004000006466, 132.8273013480001), (0....\n    \n  \n\n\n\n\nNext, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\n\n\n\n\n  \n    \n      \n      DateTime\n      Latitude\n      Longitude\n      Depth\n      Magnitude\n      MagType\n      NbStations\n      Gap\n      Distance\n      RMS\n      Source\n      EventID\n    \n  \n  \n    \n      0\n      1970-01-04 17:00:40.200\n      24.139\n      102.503\n      31.0\n      7.5\n      Ms\n      90.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970010e+09\n    \n    \n      1\n      1970-01-06 05:35:51.800\n      -9.628\n      151.458\n      8.0\n      6.2\n      Ms\n      85.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      2\n      1970-01-08 17:12:39.100\n      -34.741\n      178.568\n      179.0\n      6.1\n      Mb\n      59.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      3\n      1970-01-10 12:07:08.600\n      6.825\n      126.737\n      73.0\n      6.1\n      Mb\n      91.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      4\n      1970-01-16 08:05:39.000\n      60.280\n      -152.660\n      85.0\n      6.0\n      ML\n      0.0\n      NaN\n      NaN\n      NaN\n      AK\n      NaN\n    \n  \n\n\n\n\nThe code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=10).add_to(m_1)\n\n\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_1.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.a.solution()\n\nSo, given the map above, do earthquakes coincide with plate boundaries?\n\n# View the solution (Run this code cell to receive credit!)\n#q_1.b.solution()\n\n\n\n2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan?\nYou recently read that the depth of earthquakes tells us important information about the structure of the earth. You’re interested to see if there are any intereresting global patterns, and you’d also like to understand how depth varies in Japan.\n\nfrom branca.colormap import linear\n\ncolor_scale = linear.YlOrRd_09.scale(earthquakes['Depth'].min(), earthquakes['Depth'].max())\n\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\nfor i in range(0,len(earthquakes)):\n    Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=20,\n        color=color_scale(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n    \n\n\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# View the map\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nCan you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan?\n\n# View the solution (Run this code cell to receive credit!)\n#q_2.b.solution()\n\n\n\n3) Which prefectures have high population density?\nRun the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\japan-prefecture-boundaries\\japan-prefecture-boundaries\\japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n    \n    \n      prefecture\n      \n    \n  \n  \n    \n      Aichi\n      MULTIPOLYGON (((137.09523 34.65330, 137.09546 ...\n    \n    \n      Akita\n      MULTIPOLYGON (((139.55725 39.20330, 139.55765 ...\n    \n    \n      Aomori\n      MULTIPOLYGON (((141.39860 40.92472, 141.39806 ...\n    \n    \n      Chiba\n      MULTIPOLYGON (((139.82488 34.98967, 139.82434 ...\n    \n    \n      Ehime\n      MULTIPOLYGON (((132.55859 32.91224, 132.55904 ...\n    \n  \n\n\n\n\nThe next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\n\n\n\n\n  \n    \n      \n      population\n      area_sqkm\n      density\n    \n    \n      prefecture\n      \n      \n      \n    \n  \n  \n    \n      Tokyo\n      12868000\n      1800.614782\n      7146.448049\n    \n    \n      Kanagawa\n      8943000\n      2383.038975\n      3752.771186\n    \n    \n      Osaka\n      8801000\n      1923.151529\n      4576.342460\n    \n    \n      Aichi\n      7418000\n      5164.400005\n      1436.372085\n    \n    \n      Saitama\n      7130000\n      3794.036890\n      1879.264806\n    \n  \n\n\n\n\nUse the next code cell to create a choropleth map to visualize population density.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__, \n           data=stats['density'], \n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='Population density'\n          ).add_to(m_3)\n\n# Uncomment to see a hint\n#q_3.a.hint()\n\n# View the map\nm_3\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_3.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_3.a.solution()\n\nWhich three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you’re unfamiliar with Japanese geography, you might find this map useful to answer the questions.)\n\n# View the solution (Run this code cell to receive credit!)\n#q_3.b.solution()\n\n\n\n4) Which high-density prefecture is prone to high-magnitude earthquakes?\nCreate a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\n\n\n# Create a map\ndef color_producer(magnitude):\n    if magnitude > 6:\n        return 'forestgreen'\n    else:\n        return 'darkred'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude'],\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nm_4\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nWhich prefecture do you recommend for extra earthquake reinforcement?\n\n# View the solution (Run this code cell to receive credit!)\n#q_4.b.solution()\n\n\n\n\nKeep going\nLearn how to convert names of places to geographic coordinates with geocoding. You’ll also explore special ways to join information from multiple GeoDataFrames.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/quiz/exercise-manipulating-geospatial-data.html",
    "href": "posts/quiz/exercise-manipulating-geospatial-data.html",
    "title": "Quiz_4",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nIntroduction\nYou are a Starbucks big data analyst (that’s a real job!) looking to find the next store into a Starbucks Reserve Roastery. These roasteries are much larger than a typical Starbucks store and have several additional features, including various food and wine options, along with upscale lounge areas. You’ll investigate the demographics of various counties in the state of California, to determine potentially suitable locations.\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\n#from geopy.geocoders import Nominatim            # What you'd normally run\n#from learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex4 import *\n\nYou’ll use the embed_map() function from the previous exercise to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) Geocode the missing locations.\nRun the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California.\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\starbucks_locations.csv\")\nstarbucks.head()\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n    \n  \n\n\n\n\nMost of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nStore Number    0\nStore Name      0\nAddress         0\nCity            0\nLongitude       5\nLatitude        5\ndtype: int64\n\n\n\n\n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      153\n      5406-945\n      2224 Shattuck - Berkeley\n      2224 Shattuck Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      154\n      570-512\n      Solano Ave\n      1799 Solano Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      155\n      17877-164526\n      Safeway - Berkeley #691\n      1444 Shattuck Place Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      156\n      19864-202264\n      Telegraph & Ashby\n      3001 Telegraph Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      157\n      9217-9253\n      2128 Oxford St.\n      2128 Oxford Street Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n  \n\n\n\n\nUse the code cell below to fill in these values with the Nominatim geocoder.\nNote that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course.\nIn this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas.\nSo, in other words, as long as: - you don’t change the import statements at the top of the notebook, and - you call the geocoding function as geocode() in the code cell below,\nyour code will work as intended!\n\nfrom geopy.geocoders import Nominatim\n\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\ngeolocator\n\n# Your code here\n\n\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n# Check your answer\n#q_1.check()\n\n\n# Line below will give you solution code\n#q_1.solution()\n\n\n\n2) View Berkeley locations.\nLet’s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_2.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.a.solution()\n\nConsidering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)?\n\n# View the solution (Run this code cell to receive credit!)\n#q_2.b.solution()\n\n\n\n3) Consolidate your data.\nRun the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the “GEOID” column) for each county in the state of California. The “geometry” column contains a polygon with county boundaries.\n\nCA_counties = gpd.read_file(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\CA_county_boundaries\\CA_county_boundaries\\CA_county_boundaries.shp\")\nCA_counties.crs = {'init': 'epsg:4326'}\nCA_counties.head()\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n    \n  \n\n\n\n\nNext, we create three DataFrames: - CA_pop contains an estimate of the population of each county. - CA_high_earners contains the number of households with an income of at least $150,000 per year. - CA_median_age contains the median age for each county.\n\nCA_pop = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\CA_county_median_age.csv\", index_col=\"GEOID\")\n\nUse the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age.\nName the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners”, and “median_age”.\n\ntype(CA_median_age)\n\npandas.core.frame.DataFrame\n\n\n\nCA_pop.head()\n\n\n\n\n\n  \n    \n      \n      population\n    \n    \n      GEOID\n      \n    \n  \n  \n    \n      6001\n      1666753\n    \n    \n      6003\n      1101\n    \n    \n      6005\n      39383\n    \n    \n      6007\n      231256\n    \n    \n      6009\n      45602\n    \n  \n\n\n\n\n\nCA_high_earners.head()\n\n\n\n\n\n  \n    \n      \n      high_earners\n    \n    \n      GEOID\n      \n    \n  \n  \n    \n      6001\n      145696\n    \n    \n      6003\n      30\n    \n    \n      6005\n      1220\n    \n    \n      6007\n      6860\n    \n    \n      6009\n      2046\n    \n  \n\n\n\n\n\nCA_median_age.head()\n\n\n\n\n\n  \n    \n      \n      median_age\n    \n    \n      GEOID\n      \n    \n  \n  \n    \n      6001\n      37.3\n    \n    \n      6003\n      44.9\n    \n    \n      6005\n      50.6\n    \n    \n      6007\n      36.9\n    \n    \n      6009\n      51.6\n    \n  \n\n\n\n\n\n# Your code here\na = CA_pop.join([CA_high_earners, CA_median_age])\nCA_stats =CA_counties.merge(a, on = \"GEOID\")\n\n# Check your answer\n#q_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nNow that we have all of the data in one place, it’s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a “density” column with the population density.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) Which counties look promising?\nCollapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria.\nUse the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where: - there are at least 100,000 households making $150,000 per year, - the median age is less than 38.5, and - the density of inhabitants is at least 285 (per square kilometer).\nAdditionally, selected counties should satisfy at least one of the following criteria: - there are at least 500,000 households making $150,000 per year, - the median age is less than 35.5, or - the density of inhabitants is at least 1400 (per square kilometer).\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners > 100000) &\n                         (CA_stats.median_age < 38.5) &\n                         (CA_stats.density > 285) &\n                         ((CA_stats.median_age < 35.5) |\n                         (CA_stats.density > 1400) |\n                         (CA_stats.high_earners > 500000)))]\nsel_counties\n\n# Check your answer\n#q_4.check()\n\n\n\n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n      population\n      high_earners\n      median_age\n      density\n    \n  \n  \n    \n      5\n      6037\n      Los Angeles County\n      12305.376879\n      MULTIPOLYGON (((-118.66761 33.47749, -118.6682...\n      10105518\n      501413\n      36.0\n      821.227834\n    \n    \n      8\n      6073\n      San Diego County\n      11721.342229\n      POLYGON ((-117.43744 33.17953, -117.44955 33.1...\n      3343364\n      194676\n      35.4\n      285.237299\n    \n    \n      10\n      6075\n      San Francisco County\n      600.588247\n      MULTIPOLYGON (((-122.60025 37.80249, -122.6123...\n      883305\n      114989\n      38.3\n      1470.733077\n    \n  \n\n\n\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) How many stores did you identify?\nWhen looking for the next Starbucks Reserve Roastery location, you’d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties?\nTo prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\nSo, how many stores are in the counties you selected?\n\n# Fill in your answer\nst_county = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(st_county)\nnum_stores\n\n# Check your answer\n#q_5.check()\n\n1043\n\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Visualize the store locations.\nCreate a map that shows the locations of the stores that you identified in the previous question.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nfor idx, row in st_county.iterrows():\n    Marker([row['Latitude'], row['Longitude']], popup=row['City']).add_to(m_6)\n\n# Uncomment to see a hint\n#q_6.hint()\n\n# Show the map\nm_6\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n\n#q_6.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_6.solution()\n\n\n\n\nKeep going\nLearn about how proximity analysis can help you to understand the relationships between points on a map.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/quiz/exercise-proximity-analysis.html",
    "href": "posts/quiz/exercise-proximity-analysis.html",
    "title": "Quiz_5",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nIntroduction\nYou are part of a crisis response team, and you want to identify how hospitals have been responding to crash collisions in New York City.\n\n\n\nBefore you get started, run the code cell below to set everything up.\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex5 import *\n\nYou’ll use the embed_map() function to visualize your maps.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\nExercises\n\n1) Visualize the collision data.\nRun the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018.\n\ncollisions = gpd.read_file(\"C:/Users/ENGLISH/Desktop/archive/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\nUse the “LATITUDE” and “LONGITUDE” columns to create an interactive map to visualize the collision data. What type of map do you think is most effective?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data = collisions[[\"LATITUDE\", \"LONGITUDE\"]], radius = 9).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.hint()\n\n# Show the map\nm_1\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_1.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_1.solution()\n\n\n\n2) Understand hospital coverage.\nRun the next code cell to load the hospital data.\n\nhospitals = gpd.read_file(r\"C:\\Users\\ENGLISH\\Desktop\\archive\\nyu_2451_34494\\nyu_2451_34494\\nyu_2451_34494.shp\")\nhospitals.head()\n\n\n\n\n\n  \n    \n      \n      id\n      name\n      address\n      zip\n      factype\n      facname\n      capacity\n      capname\n      bcode\n      xcoord\n      ycoord\n      latitude\n      longitude\n      geometry\n    \n  \n  \n    \n      0\n      317000001H1178\n      BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI...\n      1650 Grand Concourse\n      10457\n      3102\n      Hospital\n      415\n      Beds\n      36005\n      1008872.0\n      246596.0\n      40.843490\n      -73.911010\n      POINT (1008872.000 246596.000)\n    \n    \n      1\n      317000001H1164\n      BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION\n      1276 Fulton Ave\n      10456\n      3102\n      Hospital\n      164\n      Beds\n      36005\n      1011044.0\n      242204.0\n      40.831429\n      -73.903178\n      POINT (1011044.000 242204.000)\n    \n    \n      2\n      317000011H1175\n      CALVARY HOSPITAL INC\n      1740-70 Eastchester Rd\n      10461\n      3102\n      Hospital\n      225\n      Beds\n      36005\n      1027505.0\n      248287.0\n      40.848060\n      -73.843656\n      POINT (1027505.000 248287.000)\n    \n    \n      3\n      317000002H1165\n      JACOBI MEDICAL CENTER\n      1400 Pelham Pkwy\n      10461\n      3102\n      Hospital\n      457\n      Beds\n      36005\n      1027042.0\n      251065.0\n      40.855687\n      -73.845311\n      POINT (1027042.000 251065.000)\n    \n    \n      4\n      317000008H1172\n      LINCOLN MEDICAL & MENTAL HEALTH CENTER\n      234 E 149 St\n      10451\n      3102\n      Hospital\n      362\n      Beds\n      36005\n      1005154.0\n      236853.0\n      40.816758\n      -73.924478\n      POINT (1005154.000 236853.000)\n    \n  \n\n\n\n\nUse the “latitude” and “longitude” columns to visualize the hospital locations.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.hint()\n        \n# Show the map\nm_2\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Get credit for your work after you have created a map\n#q_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) When was the closest hospital more than 10 kilometers away?\nCreate a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital.\nNote that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters.\n\n# Your code here\nh_p = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nu = h_p.geometry.unary_union\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: u.contains(x))]\n\n# Check your answer\n#q_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\nThe next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital.\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\nPercentage of collisions more than 10 km away from the closest hospital: 15.12%\n\n\n\n\n4) Make a recommender.\nWhen collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital.\nWith this in mind, you decide to create a recommender that: - takes the location of the crash (in EPSG 2263) as input, - finds the closest hospital (where distance calculations are done in EPSG 2263), and - returns the name of the closest hospital.\n\ndef best_hospital(collision_location):\n    m = hospitals.geometry.distance(collision_location).idxmin()\n    hp = hospitals.iloc[m]\n    name = hp[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\n# Check your answer\n#q_4.check()\n\nCALVARY HOSPITAL INC\n\n\n\n# Lines below will give you a hint or solution code\n#q_4.hint()\n#q_4.solution()\n\n\n\n5) Which hospital is under the highest demand?\nConsidering only collisions in the outside_range DataFrame, which hospital is most recommended?\nYour answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4).\n\n# Your code here\n#highest_demand \n\n# Check your answer\n#q_5.check()\n\n\n# Lines below will give you a hint or solution code\n#q_5.hint()\n#q_5.solution()\n\n\n\n6) Where should the city construct new hospitals?\nRun the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nm_6\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\nClick anywhere on the map to see a pop-up with the corresponding location in latitude and longitude.\nThe city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal?\nPut the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.)\nThen, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.7\nlong_1 = -74\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.7\nlong_2 = -74\n\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    \n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(m)\nexcept:\n    0\n\nc:\\Users\\ENGLISH\\.conda\\envs\\myenv\\Lib\\site-packages\\pyproj\\crs\\crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n(NEW) Percentage of collisions more than 10 km away from the closest hospital: 15.12%\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# Uncomment to see one potential answer \n#q_6.solution()\n\n\n\n\nCongratulations!\nYou have just completed the Geospatial Analysis micro-course! Great job!\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/quiz/exercise_first_map.html",
    "href": "posts/quiz/exercise_first_map.html",
    "title": "Quiz_1",
    "section": "",
    "text": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link.\n\n\nIntroduction\nKiva.org is an online crowdfunding platform extending financial services to poor people around the world. Kiva lenders have provided over $1 billion dollars in loans to over 2 million people.\n\n\n\nKiva reaches some of the most remote places in the world through their global network of “Field Partners”. These partners are local organizations working in communities to vet borrowers, provide services, and administer loans.\nIn this exercise, you’ll investigate Kiva loans in the Philippines. Can you identify regions that might be outside of Kiva’s current network, in order to identify opportunities for recruiting new Field Partners?\nTo get started, run the code cell below to set up our feedback system.\n\nimport geopandas as gpd\n\n#from learntools.core import binder\n#binder.bind(globals())\n#from learntools.geospatial.ex1 import *\n\n\n1) Get the data.\nUse the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans.\n\nloans_filepath = \"C:/Users/ENGLISH/Downloads/archive/kiva_loans/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\nworld_loans.head()\n\n# Check your answer\n#q_1.check()\n\n# Uncomment to view the first five rows of the data\n#world_loans.head()\n\n\n\n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n  \n\n\n\n\n\n# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()\n\n\n\n2) Plot the data.\nRun the next code cell without changes to load a GeoDataFrame world containing country boundaries.\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\nworld.head()\n\n\n\n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n  \n\n\n\n\nUse the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world.\n\n# Your code here\nax = world.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\nworld_loans.plot(color = \"green\",  ax = ax, markersize = 1)\n# Uncomment to see a hint\n#q_2.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_2.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_2.solution()\n\n\n\n3) Select loans based in the Philippines.\nNext, you’ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines.\n\n# Your code here\nPHL_loans = world_loans[world_loans['country'] == \"Philippines\"]\n\n# Check your answer\n#q_3.check()\n\n\n# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()\n\n\n\n4) Understand loans in the Philippines.\nRun the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"C:/Users/ENGLISH/Downloads/archive/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\n\n\n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Autonomous Region in Muslim Mindanao\n      \n      MULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n    \n    \n      1\n      Bicol Region\n      \n      MULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n    \n    \n      2\n      Cagayan Valley\n      \n      MULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n    \n    \n      3\n      Calabarzon\n      \n      MULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n    \n    \n      4\n      Caraga\n      \n      MULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n    \n  \n\n\n\n\nUse the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines.\n\n# Your code here\nax = world.plot(figsize=(10,10), color='none', edgecolor='gainsboro', zorder=3)\nworld_loans.plot(color = \"green\",  ax = ax, markersize = 1)\n\n# Uncomment to see a hint\n#q_4.a.hint()\n\n<Axes: >\n\n\n\n\n\n\n# Get credit for your work after you have created a map\n#q_4.a.check()\n\n# Uncomment to see our solution (your code may look different!)\n#q_4.a.solution()\n\nCan you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva’s reach?\nYou might find this map useful to answer the question.\n\n# View the solution (Run this code cell to receive credit!)\n#q_4.b.solution()\n\n\n\n\nKeep going\nContinue to learn about coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]